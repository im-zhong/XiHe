{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a0520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/25\n",
    "# zhangzhong\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "323229d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddcea9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['a'],\n",
      "    num_shards: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = Dataset.from_dict({\"a\": range(64)}).to_iterable_dataset(\n",
    "    num_shards=4\n",
    ")\n",
    "print(iterable_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9b19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0}\n",
      "{'a': 1}\n",
      "{'a': 2}\n",
      "{'a': 3}\n",
      "{'a': 4}\n",
      "{'a': 5}\n",
      "{'a': 6}\n",
      "{'a': 7}\n",
      "{'a': 8}\n",
      "{'a': 9}\n",
      "{'a': 10}\n",
      "{'a': 11}\n",
      "{'a': 12}\n",
      "{'a': 13}\n",
      "{'a': 14}\n",
      "{'a': 15}\n"
     ]
    }
   ],
   "source": [
    "ds1 = iterable_dataset.shard(num_shards=4, index=0)\n",
    "for example in ds1:\n",
    "    print(example)\n",
    "\n",
    "# split_dataset_by_node\n",
    "# For iterable datasets:\n",
    "# If the dataset has a number of shards that is a factor of world_size (i.e. if dataset.num_shards % world_size == 0), \n",
    "# then the shards are evenly assigned across the nodes, which is the most optimized. \n",
    "# Otherwise, each node keeps 1 example out of world_size, skipping the other examples.\n",
    "# 换句话说，就是“每 4 个样本取一个”，且不同的进程负责不同的样本索引。\n",
    "\n",
    "# 每个dataloader都需要记录自己的状态\n",
    "# 他们在不同的进程里面是不一样的\n",
    "# 如果可以做到只保留一个就很好\n",
    "# 因为不同的进程分别保存自己的checkpoint比较麻烦\n",
    "# 在笔记上画一下数据构造的全部流程吧\n",
    "# 看看有什么什么突破口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df1be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['a'],\n",
      "    num_shards: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = split_dataset_by_node(iterable_dataset, rank=0, world_size=4)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f266959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0}\n",
      "{'a': 1}\n",
      "{'a': 2}\n",
      "{'a': 3}\n",
      "{'a': 4}\n",
      "{'a': 5}\n",
      "{'a': 6}\n",
      "{'a': 7}\n",
      "{'a': 8}\n",
      "{'a': 9}\n",
      "{'a': 10}\n",
      "{'a': 11}\n",
      "{'a': 12}\n",
      "{'a': 13}\n",
      "{'a': 14}\n",
      "{'a': 15}\n"
     ]
    }
   ],
   "source": [
    "# 要跟DDP结合啊\n",
    "for example in dataset:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965b6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['a'],\n",
      "    num_shards: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "iterable_dataset = Dataset.from_dict({\"a\": range(64)}).to_iterable_dataset(\n",
    "    num_shards=4\n",
    ")\n",
    "print(iterable_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "159dbe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'a': 0}, {'a': 16}, {'a': 32}, {'a': 48})\n"
     ]
    }
   ],
   "source": [
    "ds1 = split_dataset_by_node(iterable_dataset, rank=0, world_size=4)\n",
    "ds2 = split_dataset_by_node(iterable_dataset, rank=1, world_size=4)\n",
    "ds3 = split_dataset_by_node(iterable_dataset, rank=2, world_size=4)\n",
    "ds4 = split_dataset_by_node(iterable_dataset, rank=3, world_size=4)\n",
    "\n",
    "for example in zip(ds1, ds2, ds3, ds4):\n",
    "    print(example)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a6e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'a': 0}, {'a': 16}, {'a': 32}, {'a': 48})\n"
     ]
    }
   ],
   "source": [
    "# save the state dict of iterable_dataset\n",
    "state_dict = iterable_dataset.state_dict()\n",
    "iterable_dataset = Dataset.from_dict({\"a\": range(64)}).to_iterable_dataset(\n",
    "    num_shards=4\n",
    ")\n",
    "iterable_dataset.load_state_dict(state_dict)\n",
    "\n",
    "ds1 = split_dataset_by_node(iterable_dataset, rank=0, world_size=4)\n",
    "ds2 = split_dataset_by_node(iterable_dataset, rank=1, world_size=4)\n",
    "ds3 = split_dataset_by_node(iterable_dataset, rank=2, world_size=4)\n",
    "ds4 = split_dataset_by_node(iterable_dataset, rank=3, world_size=4)\n",
    "\n",
    "for example in zip(ds1, ds2, ds3, ds4):\n",
    "    print(example)\n",
    "    break\n",
    "\n",
    "# 没用，可以看到iterable的dataset，新加上去的层不会影响之前的层\n",
    "# 所以最合适的方法，就是使用stateful dataloader就可以le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035736df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tensor([0, 1, 2, 3, 4, 5, 6, 7])}\n",
      "{'a': tensor([ 8,  9, 10, 11, 12, 13, 14, 15])}\n"
     ]
    }
   ],
   "source": [
    "dataloader = StatefulDataLoader(ds1, batch_size=8)\n",
    "for example in dataloader:\n",
    "    print(example)\n",
    "    break\n",
    "\n",
    "state_dict = dataloader.state_dict()\n",
    "\n",
    "# 这里就是可以正常工作的\n",
    "\n",
    "iterable_dataset = Dataset.from_dict({\"a\": range(64)}).to_iterable_dataset(\n",
    "    num_shards=4\n",
    ")\n",
    "ds1 = split_dataset_by_node(iterable_dataset, rank=0, world_size=4)\n",
    "dataloader = StatefulDataLoader(ds1, batch_size=8)\n",
    "dataloader.load_state_dict(state_dict)\n",
    "for example in dataloader:\n",
    "    print(example)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be73d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所以这个dataloder目前看来只能是每个进程自己保存一份自己的state_dict了\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xihe (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
